{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8tBn78EmfT3"
      },
      "source": [
        "## Housing project_Regression\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn import set_config\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "\n",
        "path = r\"C:\\Users\\Aida\\OneDrive\\Documents\\Bootcamp_WBS\\Primer\\Python\\WBS_DATA\\8_SUP_ML\\Regression\\Data\\housing_iteration_6_regression.csv\"\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "\n",
        "# X: All columns except 'SalePrice' (features)\n",
        "X = data.drop(columns=['SalePrice'])\n",
        "\n",
        "# y: The 'SalePrice' column (target)\n",
        "y = data['SalePrice']\n",
        "\n",
        "# data splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=31416)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build preprocessor\n",
        "X_cat = X_train.select_dtypes(exclude=\"number\").copy()\n",
        "X_num = X_train.select_dtypes(include=\"number\").copy()\n",
        "\n",
        "# We have to take extra steps to maintain feature names if we want to know their coefficients\n",
        "# Encoders and ColumnTransformers will remove feature names unless .set_output is specified\n",
        "\n",
        "# Step 1: Defining ordinal & onehot columns\n",
        "ordinal_cols = [\n",
        "    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'KitchenQual', 'FireplaceQu', \n",
        "    'PoolQC', 'HeatingQC', 'GarageFinish', 'GarageQual',\n",
        "    'GarageCond', 'PavedDrive','LandSlope', 'LotShape', 'OverallCond', 'OverallQual'\n",
        "]\n",
        "\n",
        "onehot_cols = [\n",
        "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LandContour', 'Utilities', 'LotConfig', \n",
        "    'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', \n",
        "    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', \n",
        "    'CentralAir', 'Electrical', 'GarageType', 'Fence', 'SaleType', 'SaleCondition'\n",
        "]\n",
        "\n",
        "# Step 2: Defining the categorical encoder (with \"N_A\" for missing values)\n",
        "# Corrected ordinal_categories\n",
        "ordinal_categories = [\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # ExterQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # ExterCond\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # BsmtQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # BsmtCond\n",
        "    ['Gd', 'Av', 'Mn', 'No'],        # BsmtExposure\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # KitchenQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # FireplaceQu\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # PoolQC\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # HeatingQC\n",
        "    ['Fin', 'RFn', 'Unf'],           # GarageFinish\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # GarageQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # GarageCond\n",
        "    ['Y', 'P', 'N'],                 # PavedDrive\n",
        "    ['Gtl', 'Mod', 'Sev'],           # LandSlope\n",
        "    ['Reg', 'IR1', 'IR2', 'IR3'],    # LotShape\n",
        "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1], # OverallCond\n",
        "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]  # OverallQual\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# === Preprocessing Pipelines ===\n",
        "\n",
        "# Ordinal pipeline\n",
        "ordinal_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=\"N_A\")),  # Fill missing values\n",
        "    ('ordinal', OrdinalEncoder(categories=ordinal_categories, handle_unknown='use_encoded_value', unknown_value=-1))  # Ordinal encoding\n",
        "])\n",
        "\n",
        "# One-hot pipeline\n",
        "onehot_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=\"N_A\")),  # Fill missing values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
        "])\n",
        "\n",
        "# Numerical pipeline\n",
        "numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with mean\n",
        "    ('scaler', StandardScaler())  # Standard scaling\n",
        "])\n",
        "\n",
        "# Combine pipelines with ColumnTransformer\n",
        "full_preprocessing = ColumnTransformer(transformers=[\n",
        "    (\"num_pipe\", numeric_pipeline, X_num.columns),       # Apply numerical pipeline to numerical columns\n",
        "    (\"ordinal\", ordinal_pipeline, ordinal_cols),        # Apply ordinal pipeline to ordinal columns\n",
        "    (\"onehot\", onehot_pipeline, onehot_cols)            # Apply one-hot pipeline to categorical columns\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Full pipeline: preprocessing + DecisionTreeRegressor ===\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", full_preprocessing),  # Preprocessing\n",
        "    (\"decisiontreeregressor\", DecisionTreeRegressor(random_state=42))  # Model\n",
        "])\n",
        "\n",
        "# === Hyperparameter Grid ===\n",
        "param_grid = {\n",
        "    \"decisiontreeregressor__max_depth\": [5, 10, 15, 20, None],  # Maximum depth of the tree\n",
        "    \"decisiontreeregressor__min_samples_split\": [2, 5, 10, 15],  # Minimum samples to split a node\n",
        "    \"decisiontreeregressor__min_samples_leaf\": [1, 2, 5, 10],  # Minimum samples in a leaf\n",
        "    \"decisiontreeregressor__max_features\": [\"sqrt\", \"log2\", None]  # Features considered at each split\n",
        "}\n",
        "\n",
        "# === GridSearchCV for Hyperparameter Tuning ===\n",
        "search = GridSearchCV(\n",
        "    full_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    verbose=1,\n",
        "    scoring=\"r2\",  # Optimize for R^2\n",
        "    n_jobs=-1  # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# === Model Training ===\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# === Results ===\n",
        "print(\"Best Parameters:\", search.best_params_)\n",
        "print(\"Best Score (RÂ²):\", search.best_score_)\n",
        "\n",
        "# === Feature Importance Analysis ===\n",
        "# Extract the best decision tree model\n",
        "best_model = search.best_estimator_.named_steps['decisiontreeregressor']\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = best_model.feature_importances_\n",
        "\n",
        "# Retrieve feature names from preprocessing pipeline\n",
        "numeric_features = X_num.columns.tolist()  # Numerical feature names\n",
        "ordinal_features = ordinal_cols  # Ordinal feature names\n",
        "onehot_features = search.best_estimator_.named_steps['preprocessor'] \\\n",
        "    .transformers_[2][1].named_steps['onehot'].get_feature_names_out(onehot_cols).tolist()\n",
        "\n",
        "# Combine all feature names\n",
        "all_feature_names = numeric_features + ordinal_features + onehot_features\n",
        "\n",
        "# Create a dictionary for feature importances\n",
        "importance_dict = dict(zip(all_feature_names, feature_importances))\n",
        "\n",
        "# Sort and display feature importances\n",
        "sorted_importances = sorted(importance_dict.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "print(\"Feature Importances (sorted):\")\n",
        "for feature, importance in sorted_importances:\n",
        "    print(f\"{feature}: {importance}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RÂ² Score : 0.979832375807639\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Make predictions on the training data\n",
        "y_train_pred = search.predict(X_train)\n",
        "\n",
        "# Calculate accuracy score\n",
        "r2 = r2_score(y_train, y_train_pred )\n",
        "print(f\"RÂ² Score : {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RÂ² Score : 0.7215943880705856\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = search.predict(X_test)  # Predictions on test data\n",
        "r2 = r2_score(y_test, y_test_pred) \n",
        "print(f\"RÂ² Score : {r2}\")   # Compare to test labels"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
